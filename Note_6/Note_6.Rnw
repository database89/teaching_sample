\documentclass{article}
\usepackage{amsmath}
\newtheorem{exmp}{Example}[section]
\begin{document}
\SweaveOpts{concordance=TRUE}
\title{Forecasting}
\date{}
\maketitle
\section{Overview of forecast problems}
The following contents are based the book (Rob J Hyndman and George Athanasopoulos: Forecasting: Principles and Practice).
\subsection{What can be forecast?}
Forecasting is required in many situations: deciding whether to build another power generation plant in the next five years requires forecasts of future demand; scheduling staff in a call centre next week requires forecasts of call volumes; stocking an inventory requires forecasts of stock requirements. Forecasts can be required several years in advance (for the case of capital investments), or only a few minutes beforehand (for telecommunication routing). Whatever the circumstances or time horizons involved, forecasting is an important aid to effective and efficient planning.

Some things are easier to forecast than others. The time of the sunrise tomorrow morning can be forecast precisely. On the other hand, tomorrow's lotto numbers cannot be forecast with any accuracy. The predictability of an event or a quantity depends on several factors including:
\begin{itemize}
  \item how well we understand the factors that contribute to it;
  \item how much data are available;
  \item whether the forecasts can affect the thing we are trying to forecast.
\end{itemize}
Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again. 

Examples: \begin{itemize}
  \item Forecasts of electricity demand can be highly accurate because all three conditions are usually satisfied. We have a good idea of the contributing factors: electricity demand is driven largely by temperatures, with smaller effects for calendar variation such as holidays, and economic conditions. Provided there is a sufficient history of data on electricity demand and weather conditions, and we have the skills to develop a good model linking electricity demand and the key driver variables, the forecasts can be remarkably accurate.
  \item Forecasting currency exchange rates, only one of the conditions is satisfied: there is plenty of available data. However, we have a limited understanding of the factors that affect exchange rates, and forecasts of the exchange rate have a direct effect on the rates themselves. If there are well-publicised forecasts that the exchange rate will increase, then people will immediately adjust the price they are willing to pay and so the forecasts are self-fulfilling. In a sense, the exchange rates become their own forecasts.  In situations like this, forecasters need to be aware of their own limitations, and not claim more than is possible.
\end{itemize}

Many people wrongly assume that forecasts are not possible in a changing environment. Every environment is changing, and a good forecasting model captures the way in which things are changing. Forecasts rarely assume that the environment is unchanging. What is normally assumed is that the way in which the environment is changing will continue into the future. 
\begin{itemize}
  \item Qualitative forecasting: if there are no data available, or if the data available are not relevant to the forecasts, then qualitative forecasting methods must be used. These methods are not purely guesswork-there are well-developed structured approaches to obtaining good forecasts without using historical data. 
  \item Quantitative forecasting can be applied when two conditions are satisfied: (1) numerical information about the past is available; (2) it is reasonable to assume that some aspects of the past patterns will continue into the future. Most quantitative prediction problems use either time series data (collected at regular intervals over time) or cross-sectional data (collected at a single point in time).
\end{itemize}

Forecasting is a common statistical task in business, where it helps to inform decisions about the scheduling of production, transportation and personnel, and provides a guide to long-term strategic planning. 

\begin{itemize}
  \item Short-term forecasts are needed for the scheduling of personnel, production and transportation. As part of the scheduling process, forecasts of demand are often also required.
  \item Medium-term forecasts are needed to determine future resource requirements, in order to purchase raw materials, hire personnel, or buy machinery and equipment.
  \item Long-term forecasts
are used in strategic planning. Such decisions must take account of market opportunities, environmental factors and internal resources.
\end{itemize}




\subsection{Basic steps in a forecasting task}
\begin{itemize}
  \item Step 1: Problem definition. Often this is the most difficult part of forecasting. Defining the problem carefully requires an understanding of the way the forecasts will be used, who requires the forecasts, and how the forecasting function fits within the organisation requiring the forecasts. A forecaster needs to spend time talking to everyone who will be involved in collecting data, maintaining databases, and using the forecasts for future planning.
  \item Step 2: Gathering information.
There are always at least two kinds of information required: (a) statistical data, and (b) the accumulated expertise of the people who collect the data and use the forecasts. Often, it will be difficult to obtain enough historical data to be able to fit a good statistical model. In that case, the judgmental forecasting methods can be used. Occasionally, old data will be less useful due to structural changes in the system being forecast; then we may choose to use only the most recent data. However, remember that good statistical models will handle evolutionary changes in the system; don't throw away good data unnecessarily.
\item Step 3: Preliminary (exploratory) analysis.
Always start by graphing the data. Are there consistent patterns? Is there a significant trend? Is seasonality important? Is there evidence of the presence of business cycles? Are there any outliers in the data that need to be explained by those with expert knowledge? How strong are the relationships among the variables available for analysis? Various tools have been developed to help with this analysis.
\item Step 4: Choosing and fitting models.
The best model to use depends on the availability of historical data, the strength of relationships between the forecast variable and any explanatory variables, and the way in which the forecasts are to be used. It is common to compare two or three potential models. Each model is itself an artificial construct that is based on a set of assumptions (explicit and implicit) and usually involves one or more parameters which must be estimated using the known historical data.
\item Step 5: Using and evaluating a forecasting model.
Once a model has been selected and its parameters estimated, the model is used to make forecasts. The performance of the model can only be properly evaluated after the data for the forecast period have become available. A number of methods have been developed to help in assessing the accuracy of forecasts. There are also organisational issues in using and acting on the forecasts.
\end{itemize}

\section{Some simple forecasting methods}
Some forecasting methods are extremely simple and surprisingly effective. 
\begin{itemize}
  \item Average method: Here, the forecasts of all future values are equal to the average (or ``mean'') of the historical data. If we let the historical data be denoted by $x_1,\dots,x_n$, then we can write the forecasts as $\tilde{x}_{n+m} = \bar{x}$. The forecast standard deviation $\sqrt{\widehat{P_{n+m}^n}} = \hat{\sigma}\sqrt{1+1/n}$ where $\hat{\sigma}$ is residual standard deviation, i.e. standard deviation of $x_t-\tilde{x}_{t|t-1}$.
  \item Naive method: we simply set all forecasts to be the value of the last observation. That is, $\tilde{x}_{n+m} = x_n.$ This method works remarkably well for many economic and financial time series. Because a naïve forecast is optimal when data follow a random walk, these are also called random walk forecasts. The forecast standard deviation $\sqrt{\widehat{P_{n+m}^n}} = \hat{\sigma}\sqrt{m}$.
  \item Seasonal naive method: A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season of the year (e.g., the same month of the previous year). The forecast standard deviation $\sqrt{\widehat{P_{n+m}^n}} = \hat{\sigma}\sqrt{k+1}$ where $k$ is the integer part of $(m-1)/s$ and $s$ is the length of the seasnal cycle.
  \item Drift method: a variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data. Thus the forecast for time $n+m$ is \[\tilde{x}_{n+m} = x_n+\frac{m}{n-1}\sum_{t=2}^n(x_t-x_{t-1}).\]
  This is equivalent to drawing a line between the first and last observations, and extrapolating it into the future. The forecast standard deviation $\sqrt{\widehat{P_{n+m}^n}} = \hat{\sigma}\sqrt{m(1+m/(n-1))}$.
\end{itemize}

\vspace*{5in}
\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(ausbeer)
beer2 <- window(ausbeer,start=1992,end=c(2007,4))
# Plot some forecasts
autoplot(beer2) +
  autolayer(meanf(beer2, h=11),
    series="Mean", PI=FALSE) +
  autolayer(naive(beer2, h=11),
    series="Naïve", PI=FALSE) +
  autolayer(snaive(beer2, h=11),
    series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))
@
\caption{Forecasts for quarterly beer production.}
\label{fig:forecast1}
\end{figure}


\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(goog200)
autoplot(goog200) +
  autolayer(meanf(goog200, h=40),
    series="Mean", PI=FALSE) +
  autolayer(rwf(goog200, h=40),
    series="Naïve", PI=FALSE) +
  autolayer(rwf(goog200, drift=TRUE, h=40),
    series="Drift", PI=FALSE) +
  ggtitle("Google stock (daily ending 6 Dec 2013)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour=guide_legend(title="Forecast"))
@
\caption{Forecasts for google daily closing stock price.}
\label{fig:forecast2}
\end{figure}

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(goog200)
autoplot(naive(goog200))
@
\caption{Naive forecasts for google daily closing stock price.}
\label{fig:forecast3}
\end{figure}
\section{Transformations and adjustments}
Adjusting the historical data can often lead to a simpler forecasting task. Here, we deal with four kinds of adjustments: calendar adjustments, population adjustments, inflation adjustments and mathematical transformations. The purpose of these adjustments and transformations is to simplify the patterns in the historical data by removing known sources of variation or by making the pattern more consistent across the whole data set. Simpler patterns usually lead to more accurate forecasts.

\subsection{Calendar adjustments}
Some of the variation seen in seasonal data may be due to simple calendar effects. In such cases, it is usually much easier to remove the variation before fitting a forecasting model. The monthdays() function in package ``forecast'' will compute the number of days in each month or quarter. 

For example, if you are studying the monthly milk production on a farm, there will be variation between the months simply because of the different numbers of days in each month, in addition to the seasonal variation across the year.

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(milk)
dframe <- cbind(Monthly = milk,
                DailyAverage = milk/monthdays(milk))
  autoplot(dframe, facet=TRUE) +
    xlab("Years") + ylab("Pounds") +
    ggtitle("Milk production per cow")
@
\caption{Milk production.}
\label{fig:forecast_milk}
\end{figure}
Based on Figure \ref{fig:forecast_milk}, notice how much simpler the seasonal pattern is in the average daily production plot compared to the average monthly production plot. By looking at the average daily production instead of the average monthly production, we effectively remove the variation due to the different month lengths. Simpler patterns are usually easier to model and lead to more accurate forecasts.

A similar adjustment can be done for sales data when the number of trading days in each month varies. In this case, the sales per trading day can be modelled instead of the total sales for each month.

\subsection{Population adjustments}
Any data that are affected by population changes can be adjusted to give per-capita data. That is, consider the data per person (or per thousand people, or per million people) rather than the total. 

For example, if you are studying the number of hospital beds in a particular region over time, the results are much easier to interpret if you remove the effects of population changes by considering the number of beds per thousand people. Then you can see whether there have been real increases in the number of beds, or whether the increases are due entirely to population increases.

\subsection{Inflation adjustments}

Data which are affected by the value of money are best adjusted before modelling. For example, the average cost of a new house will have increased over the last few decades due to inflation. A \$200,000 house this year is not the same as a \$200,000 house twenty years ago. For this reason, financial time series are usually adjusted so that all values are stated in dollar values from a particular year. For example, the house price data may be stated in year 2000 dollars.

To make these adjustments, a price index is used. If $z_t$ denotes the price index and $y_t$ denotes the original house price in year $t$, then $x_t = y_t/z_t*z_{2000}$ gives the adjusted house price at year 2000 dollar values. Price indexes are often constructed by government agencies. For consumer goods, a common price index is the Consumer Price Index (or CPI).

\subsection{Mathematical transformations}

\begin{itemize}
  \item If the data show variation that increases or decreases with the level of the series, then a transformation can be useful. For example, a logarithmic transformation is often useful. Logarithms are useful because they are interpretable: changes in a log value are relative (or percentage) changes on the original scale. So if log base 10 is used, then an increase of 1 on the log scale corresponds to a multiplication of 10 on the original scale. Denote the original observations as $y_1,\dots y_n$, and the transformed observations as $x_t$.
\item Power transformation: sometimes other transformations are also used (although they are not so interpretable). For example, square roots and cube roots can be used. These are called power transformations because they can be written in the form $x_t = y_t^p$. 
\item Box-Cox transformatioins: $x_t = log(y_t)$ if $\lambda=0$ and  $x_t = (y_t^{\lambda}-1)/\lambda$ otherwise. If some $y_t<0$, no power transformation is possible unless all observations are adjusted by adding a constant to all values. Choose a simple value of $\lambda$. It makes explanations easier. Transformations sometimes make little difference to the forecasts but have a large effect on prediction interval.

\end{itemize}



One issue with using mathematical transformations such as Box-Cox transformations is that the back-transformed forecast will not be the mean of the forecast distribution. In fact, it will usually be the median of the forecast distribution (assuming that the distribution on the transformed space is symmetric). For many purposes, this is acceptable, but occasionally the mean forecast is required. For example, you may wish to add up sales forecasts from various regions to form a forecast for the whole country. But medians do not add up, whereas means do.

For a Box-Cox trasnformation, the reverse Box-Cox transformation is given by

\[y_t = \left\{\begin{array}{cc}exp(x_t) & \lambda=0 \\ (\lambda x_t +1)^{1/\lambda} & \mbox{otherwise}\end{array}\right.\]


The back-trasnformed mean is given by
\[\mu_{y_t} = \left\{\begin{array}{cc}exp(\mu_{x_t})[1+\frac{\sigma^2_m}{2}] & \lambda=0 \\ (\lambda \mu_{x_t} +1)^{1/\lambda} [1+\frac{\sigma^2_m(1-\lambda)}{2(\lambda \mu_{x_t}+1)^2}] & \mbox{otherwise}\end{array}\right.\]
where $\sigma^2_m$ is the $m$-step forecast variance.

The difference between the simple back-transformed forecast and the mean given is called the bias. When we use the mean, rather than the median, we say the point forecasts have been bias-adjusted.


\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(elec)
autoplot(elec)
@
\caption{Electricity data.}
\label{fig:elec}
\end{figure}

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(elec)
(lambda <- BoxCox.lambda(elec))
#> [1] 0.2654
autoplot(BoxCox(elec,lambda))
@
\caption{Box Cox transformation of electricity.}
\label{fig:BC_elec}
\end{figure}


\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(eggs)
fc <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80,bootstrap=TRUE)
fc2 <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80,
  biasadj=TRUE,bootstrap=TRUE)
autoplot(eggs) +
  autolayer(fc, series="Simple back transformation",PI=TRUE) +
  autolayer(fc2, series="Bias adjusted", PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))
@
\caption{Forecast average annual price of eggs using the drift method with a log transformation. }
\label{fig:BC_elec2}
\end{figure}

If a transformation has been used, then the prediction interval should be computed on the transformed scale, and the end points back-transformed to give a prediction interval on the original scale. This approach preserves the probability coverage of the prediction interval, although it will no longer be symmetric around the point forecast.

The back-transformation of prediction intervals is done automatically using the functions in the ``forecast package'' in R, provided you have used the lambda argument when computing the forecasts.

\section{Evaluating forecast accuracy}

{\bf Training and test sets}

\vspace{0.1in}

It is important to evaluate forecast accuracy using genuine forecasts. Consequently, the size of the residuals is not a reliable indication of how large true forecast errors are likely to be. The accuracy of forecasts can only be determined by considering how well a model performs on new data that were not used when fitting the model.

\vspace{0.1in}

When choosing models, it is common practice to separate the available data into two portions, training and test data, where the training data is used to estimate any parameters of a forecasting method and the test data is used to evaluate its accuracy. Because the test data is not used in determining the forecasts, it should provide a reliable indication of how well the model is likely to forecast on new data.

The size of the test set is typically about {\bf 20\%} of the total sample, although this value depends on how long the sample is and how far ahead you want to forecast. The test set should ideally be at least as large as the maximum forecast horizon required. The following points should be noted.

\vspace{0.1in}

{\bf Functions to subset a time series}
\begin{itemize}
  \item window() function is useful when extracting a portion of a time series, such as we need when creating training and test sets. In the window() function, we specify the start and/or end of the portion of time series required using time values. For example, ``window(ausbeer, start=1995)'' extracts all data from 1995 onward.
  \item subset() which allows for more types of subsetting. A great advantage of this function is that it allows the use of indices to choose a subset. For example, ``subset(ausbeer, start=length(ausbeer)-4*5)'' extracts the last 5 years of observations from ausbeer. It also allows extracting all values for a specific season. For example, ``subset(ausbeer, quarter = 1)'' extracts the first quarters for all years.
\item head and tail are useful for extracting the first few or last few observations. For example, the last 5 years of ausbeer can also be obtained using ``tail(ausbeer, 4*5)''.
\end{itemize}

\vspace{0.1in}

{\bf Forecast errors}: A forecast ``error'' is the difference between an observed value and its forecast. Here ``error'' does not mean a mistake, it means the unpredictable part of an observation. It can be written as \[e_{n+m} = x_{n+m}-\tilde{x}_{n+m}\]
Note that forecast errors are different from residuals in two ways. First, residuals are calculated on the training set while forecast errors are calculated on the test set. Second, residuals are based on one-step forecasts while forecast errors can involve multi-step forecasts.

\vspace{0.1in}

{\bf Scale-dependent errors}: The forecast errors are on the same scale as the data. Accuracy measures that are based only on $e_t$ are therefore scale-dependent and cannot be used to make comparisons between series that involve different units. The two most commonly used scale-dependent measures are based on the absolute errors or squared errors:
\[\mbox{Mean absolute error: MAE} = \mbox{mean}(|e_t|) \]
\[\mbox{Root mean squared error: RMSE} = \sqrt{ \mbox{mean}(e_t^2)} \]
When comparing forecast methods applied to a single time series, or to several time series with the same units, the MAE is popular as it is easy to both understand and compute. A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean. Consequently, the RMSE is also widely used, despite being more difficult to interpret.

\vspace{0.1in}
{\bf Percentage errors}: The percentage error is given by $p_t = 100e_t/x_t$.Percentage errors have the advantage of being unit-free, and so are frequently used to compare forecast performances between data sets. The most commonly used measure is:
 \[\mbox{Mean absolute percentage error: MAPE} = \mbox{mean}(|p_t|) \]
Measures based on percentage errors have the disadvantage of being infinite or undefined if $x_t=0$ for any $t$ in the period of interest, and having extreme values if any $x_t$ is close to zero. Another problem with percentage errors that is often overlooked is that they assume the unit of measurement has a meaningful zero. For example, a percentage error makes no sense when measuring the accuracy of temperature forecasts on either the Fahrenheit or Celsius scales, because temperature has an arbitrary zero point. They also have the disadvantage that they put a heavier penalty on negative errors than on positive errors. This observation led to the use of the so-called ``symmetric'' MAPE (sMAPE) proposed by Armstrong (1978, p. 348), which was used in the $M_3$ forecasting competition. It is defined by
 \[\mbox{sMAPE} = \mbox{mean}(200|x_t-\tilde{x}_t|/(x_t+\tilde{x}_t)) \]
 
\vspace{0.1in}
{\bf Scaled errors}: Scaled errors were proposed by Hyndman and Koehler (2006) as an alternative to using percentage errors when comparing forecast accuracy across series with different units. They proposed scaling the errors based on the training MAE from a simple forecast method.

For a non-seasonal time series, a useful way to define a scaled error uses naive forecasts:
\[q_j = \frac{e_j}{\frac{1}{n-1}\sum_{t=2}^n|x_t-x_{t-1}|}\]
Because the numerator and denominator both involve values on the scale of the original data, $q_j$ is independent of the scale of the data. A scaled error is less than one if it arises from a better forecast than the average naive forecast computed on the training data. Conversel$y$, it is greater than one if the forecast is worse than the average naive forecast computed on the training data.

For seasonal time series, a scaled error can be defined using seasonal naive forecasts:
\[q_j = \frac{e_j}{\frac{1}{n-s}\sum_{t=s+1}^n|x_t-x_{t-s}|}\]
where $s$ is the length of a cycle.

The mean abosulte scaled error is \[\mbox{MASE} = \mbox{mean}(|q_j|).\]
\begin{itemize}
  \item Armstrong, J. S. (1978). Long-range forecasting: From crystal ball to computer. John Wiley and Sons. [Amazon]
\item Hyndman, R. J., and Koehler, A. B. (2006). Another look at measures of forecast accuracy. International Journal of Forecasting, 22, 679-688.
\end{itemize}

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(googfc1)
googfc1 <- meanf(goog200, h=40)
googfc2 <- rwf(goog200, h=40)
googfc3 <- rwf(goog200, drift=TRUE, h=40)
autoplot(subset(goog, end = 240)) +
  autolayer(googfc1, PI=FALSE, series="Mean") +
  autolayer(googfc2, PI=FALSE, series="Naïve") +
  autolayer(googfc3, PI=FALSE, series="Drift") +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google stock price (daily ending 6 Dec 13)") +
  guides(colour=guide_legend(title="Forecast"))
@
\caption{Forecasts of the Google stock price from 7 Dec 2013. }
\label{fig:Google_fore}
\end{figure}

<<Google stock price forecast series>>=
#ME: Mean Error
#RMSE: Root Mean Squared Error
#MAE: Mean Absolute Error
#MPE: Mean Percentage Error
#MAPE: Mean Absolute Percentage Error
#MASE: Mean Absolute Scaled Error
googtest <- window(goog, start=201, end=240)
accuracy(googfc1, googtest)
accuracy(googfc2, googtest)
accuracy(googfc3, googtest)

@

\subsection{Time series cross-validation}
A more sophisticated version of training/test sets is time series cross-validation. In this procedure, there are a series of test sets, each consisting of a single observation. The corresponding training set consists only of observations that occurred prior to the observation that forms the test set. Thus, no future observations can be used in constructing the forecast. Since it is not possible to obtain a reliable forecast based on a small training set, the earliest observations are not considered as test sets.

The forecast accuracy is computed by averaging over the test sets. This procedure is sometimes known as ``evaluation on a rolling forecasting origin'' because the ``origin'' at which the forecast is based rolls forward in time.

With time series forecasting, one-step forecasts may not be as relevant as multi-step forecasts. In this case, the cross-validation procedure based on a rolling forecasting origin can be modified to allow multi-step errors to be used. Suppose that we are interested in models that produce good 4-step-ahead forecasts. 

Time series cross-validation is implemented with the tsCV() function. In the following example, we compare the RMSE obtained via time series cross-validation with the residual RMSE.

<<Google stock price forecast series>>=
e <- tsCV(goog200, rwf, drift=TRUE, h=1)
sqrt(mean(e^2, na.rm=TRUE))
sqrt(mean(residuals(rwf(goog200, drift=TRUE))^2, na.rm=TRUE))
@

\subsection{Pipe operator}
In the above code, we are nesting functions within functions within functions, so you have to read the code from the inside out, making it difficult to understand what is being computed. Instead, we can use the pipe operator $\%>\%$ as follows.

<<Google stock price forecast series>>=
goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e
e^2 %>% mean(na.rm=TRUE) %>% sqrt()
goog200 %>% rwf(drift=TRUE) %>% residuals() -> res
res^2 %>% mean(na.rm=TRUE) %>% sqrt()
@
The left hand side of each pipe is passed as the first argument to the function on the right hand side. This is consistent with the way we read from left to right in English. When using pipes, all other arguments must be named, which also helps readability. When using pipes, it is natural to use the right arrow assignment -> rather than the left arrow. For example, the third line above can be read as ``Take the goog200 series, pass it to rwf() with drift=TRUE, compute the resulting residuals, and store them as res''.

The goog200 data, includes daily closing stock price of Google Inc from the NASDAQ exchange for 200 consecutive trading days starting on 25 February 2013. The code in Figure \ref{fig:Google_forerror} evaluates the forecasting performance of 1- to 8-step-ahead naive forecasts with tsCV(), using MSE as the forecast error measure. The plot shows that the forecast error increases as the forecast horizon increases, as we would expect.
\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
e <- tsCV(goog200, forecastfunction=naive, h=8)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()
@
\caption{Forecast error of 1- to 8-step-ahead naive forecasts. }
\label{fig:Google_forerror}
\end{figure}

\subsection{The forecast package in R}
The forecast package in R is loaded automatically whenever you load the fpp2 package. Functions that output a forecast object:

Many functions, including meanf(), naive(), snaive() and rwf(), produce output in the form of a forecast object (i.e., an object of class forecast). This allows other functions (such as autoplot()) to work consistently across a range of forecasting models.

Objects of class forecast contain information about the forecasting method, the data used, the point forecasts obtained, prediction intervals, residuals and fitted values. There are several functions designed to work with these objects including autoplot(), summary() and print().
\section{Judemental forecasts}

Reference: Rob J Hyndman and George Athanasopoulos: Forecasting: Principles and Practice.


Forecasting using judgement is common in practice. In many cases, judgmental forecasting is the only option, such as when there is a complete lack of historical data, or when a new product is being launched, or when a new competitor enters the market, or during completely new and unique market conditions. For example, in December 2012, the Australian government was the first in the world to pass legislation that banned the use of company logos on cigarette packets, and required all cigarette packets to be a dark green colour. Judgement must be applied in order to forecast the effect of such a policy, as there are no historical precedents.

There are also situations where the data are incomplete, or only become available after some delay. For example, central banks include judgement when forecasting the current level of economic activity, a procedure known as nowcasting, as GDP is only available on a quarterly basis.

Research in this area has shown that the accuracy of judgmental forecasting improves when the forecaster has (i) important domain knowledge, and (ii) more timely, up-to-date information. A judgmental approach can be quick to adjust to such changes, information or events.

Over the years, the acceptance of judgmental forecasting as a science has increased, as has the recognition of its need. More importantly, the quality of judgmental forecasts has also improved, as a direct result of recognising that improvements in judgmental forecasting can be achieved by implementing well-structured and systematic approaches. It is important to recognise that judgmental forecasting is subjective and comes with limitations. However, implementing systematic and well-structured approaches can confine these limitations and markedly improve forecast accuracy.

There are three general settings in which judgmental forecasting is used: (i) there are no available data, so that statistical methods are not applicable and judgmental forecasting is the only feasible approach; (ii) data are available, statistical forecasts are generated, and these are then adjusted using judgement; and (iii) data are available and statistical and judgmental forecasts are generated independently and then combined. When data are available, applying statistical methods (such as those discussed in other chapters of this book), is preferable and should always be used as a starting point. Statistical forecasts are generally superior to generating forecasts using only judgement. For the majority of the chapter, we focus on the first setting where no data are available, and in the last section of the book, they discuss the judgmental adjustment of statistical forecasts.

\section{Forecast with regression}
\subsection{Linear regression}
In the simplest case, the regression model allows for a linear relationship between the forecast variable $y$ and predictor variables $\mathbf{x}$.

\begin{exmp}US consumption expenditure
Figure \ref{fig:usce} and \ref{fig:usceggplot} shows time series of quarterly percentage changes (growth rates) of real personal consumption expenditure $y$, and real personal disposable income, $x$, for the US from 1970 Q1 to 2016 Q3. Figure \ref{fig:ggpairs} is a scatterplot matrix of five variables. The first column shows the relationships between the forecast variable (consumption) and each of the predictors. The scatterplots show positive relationships with income and industrial production, and negative relationships with savings and unemployment. The strength of these relationships are shown by the correlation coefficients across the first row. The remaining scatterplots and correlation coefficients show the relationships between the predictors.
\end{exmp}

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(uschange)
autoplot(uschange[,c("Consumption","Income")]) +
  ylab("% change") + xlab("Year")
@
\caption{Percentage changes in personal consumption expenditure and personal income for the US.}
\label{fig:usce}
\end{figure}

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(uschange)
uschange %>%
  as.data.frame() %>%
  ggplot(aes(x=Income, y=Consumption)) +
    ylab("Consumption (quarterly % change)") +
    xlab("Income (quarterly % change)") +
    geom_point() +
    geom_smooth(method="lm", se=FALSE)
@
\caption{Percentage changes in personal consumption expenditure and personal income for the US.}
\label{fig:usceggplot}
\end{figure}


\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(forecast)
library(fpp2)
library(ggplot2)
data(uschange)
uschange %>%
  as.data.frame() %>%
  GGally::ggpairs()
@
\caption{A scatterplot matrix of US consumption expenditure and the four predictors.}
\label{fig:ggpairs}
\end{figure}



<<Google stock price forecast series>>=
fit.consMR <- tslm(
  Consumption ~ Income + Production + Unemployment + Savings,
  data=uschange)
summary(fit.consMR)
@

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
data(uschange)
autoplot(uschange[,'Consumption'], series="Data") +
  autolayer(fitted(fit.consMR), series="Fitted") +
  xlab("Year") + ylab("") +
  ggtitle("Percent change in US consumption expenditure") +
  guides(colour=guide_legend(title=" "))
@
\caption{Time plot of actual US consumption expenditure and predicted US consumption expenditure.}
\label{fig:uscepred}
\end{figure}


\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
checkresiduals(fit.consMR)
@
\caption{Analysing the residuals from a regression model for US quarterly consumption.}
\label{fig:usceres}
\end{figure}
The time plot Figure \ref{fig:usceres} shows some changing variation over time, but is otherwise relatively unremarkable. This heteroscedasticity will potentially make the prediction interval coverage inaccurate. The histogram shows that the residuals seem to be slightly skewed, which may also affect the coverage probability of the prediction intervals. The autocorrelation plot shows a significant spike at lag 7, but it is not quite enough for the Breusch-Godfrey to be significant at the 5\% level. In any case, the autocorrelation is not particularly large, and at lag 7 it is unlikely to have any noticeable impact on the forecasts or the prediction intervals. 

\subsection{Residual plots against fitted values}
A plot of the residuals against the fitted values should also show no pattern. If a pattern is observed, there may be ``heteroscedasticity'' in the errors which means that the variance of the residuals may not be constant. If this problem occurs, a transformation of the forecast variable such as a logarithm or square root may be required. Figure \ref{fig:usceres1} shows the residuals plotted against the fitted values. The random scatter suggests the errors are homoscedastic.
\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
cbind(Fitted = fitted(fit.consMR),
      Residuals=residuals(fit.consMR)) %>%
  as.data.frame() %>%
  ggplot(aes(x=Fitted, y=Residuals)) + geom_point()
@
\caption{Scatterplots of residuals versus fitted values.}
\label{fig:usceres1}
\end{figure}

\subsection{Spurious regression}
More often than not, time series data are ``non-stationary''; that is, the values of the time series do not fluctuate around a constant mean or with a constant variance. We will deal with time series stationarity in more detail later, but here we need to address the effect that non-stationary data can have on regression models.

\begin{exmp}
Consider the two variables plotted in Figure \ref{fig:usceres1}. These appear to be related simply because they both trend upwards in the same manner. However, air passenger traffic in Australia has nothing to do with rice production in Guinea. Regressing non-stationary time series can lead to spurious regressions. The output of regressing Australian air passengers on rice production in Guinea is shown in Figure \ref{fig:usceres2}. High $R^2$ and high residual autocorrelation can be signs of spurious regression. Notice these features in the output below. Cases of spurious regression might appear to give reasonable short-term forecasts, but they will generally not continue to work into the future.
\end{exmp}

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
data(ausair)
data(guinearice)
aussies <- window(ausair, end=2011)
autoplot(cbind(aussies,guinearice),facets=TRUE)+ xlab("Year")
@
\caption{Trending time series data can appear to be related, as shown in this example where air passengers in Australia are regressed against rice production in Guinea.}
\label{fig:usceres1}
\end{figure}

<<Spurious regression>>=
aussies <- window(ausair, end=2011)
ausfit <- tslm(aussies ~ guinearice)
summary(ausfit)
@
\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
checkresiduals(ausfit)
@
\caption{Residuals from a spurious regression.}
\label{fig:usceres2}
\end{figure}
\subsection{Useful predictors}

\begin{itemize}
  \item It is common for time series data to be trending. A linear trend can be modelled by simply using time $t$
  as a predictor.
  \item A dummy variable can also be used to account for an outlier in the data. Rather than omit the outlier, a dummy variable removes its effect. In this case, the dummy variable takes value 1 for that observation and 0 everywhere else. An example is the case where a special event has occurred. For example when forecasting tourist arrivals to Brazil, we will need to account for the effect of the Rio de Janeiro summer Olympics in 2016.
  \item Suppose that we are forecasting daily data and we want to account for the day of the week as a predictor. Then the seasonal dummy variables can be created. Only six dummy variables are needed to code seven categories. That is because the seventh category (in this case Sunday) is captured by the intercept, and is specified when the dummy variables are all set to zero.
  \item It is often necessary to model interventions that may have affected the variable to be forecast. For example, competitor activity, advertising expenditure, industrial action, and so on, can all have an effect. When the effect lasts only for one period, we use a ``spike'' variable. This is a dummy variable that takes value one in the period of the intervention and zero elsewhere. A spike variable is equivalent to a dummy variable for handling an outlier. Other interventions have an immediate and permanent effect. If an intervention causes a level shift (i.e., the value of the series changes suddenly and permanently from the time of intervention), then we use a ``step'' variable. A step variable takes value zero before the intervention and one from the time of intervention onward. Another form of permanent effect is a change of slope. Here the intervention is handled using a piecewise linear trend; a trend that bends at the time of intervention and hence is nonlinear. We will discuss this later.
  \item The number of trading days in a month can vary considerably and can have a substantial effect on sales data. To allow for this, the number of trading days in each month can be included as a predictor.
  \item It is often useful to include advertising expenditure as a predictor. However, since the effect of advertising can last beyond the actual campaign, we need to include lagged values of advertising expenditure.
  \item Easter differs from most holidays because it is not held on the same date each year, and its effect can last for several days. In this case, a dummy variable can be used with value one where the holiday falls in the particular time period and zero otherwise.
  \item An alternative to using seasonal dummy variables, especially for long seasonal periods, is to use Fourier terms.
\end{itemize}

\subsection{Forecast with linear regression}
\begin{itemize}
  \item Ex-ante forecasts are those that are made using only the information that is available in advance. For example, ex-ante forecasts for the percentage change in US consumption for quarters following the end of the sample, should only use information that was available up to and including 2016 Q3. Therefore in order to generate ex-ante forecasts, the model requires forecasts of the predictors.
  \item Ex-post forecasts are those that are made using later information on the predictors. For example, ex-post forecasts of consumption may use the actual observations of the predictors, once these have been observed. These are not genuine forecasts, but are useful for studying the behaviour of forecasting models.
\end{itemize}
A comparative evaluation of ex-ante forecasts and ex-post forecasts can help to separate out the sources of forecast uncertainty. This will show whether forecast errors have arisen due to poor forecasts of the predictor or due to a poor forecasting model.

\begin{exmp} Australian quarterly beer production: Normally, we cannot use actual future values of the predictor variables when producing ex-ante forecasts because their values will not be known in advance. However, the special predictors are all known in advance, as they are based on calendar variables (e.g., seasonal dummy variables or public holiday indicators) or deterministic functions of time (e.g. time trend). In such cases, there is no difference between ex-ante and ex-post forecasts.
\end{exmp}
\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
beer2 <- window(ausbeer, start=1992)
fit.beer <- tslm(beer2 ~ trend + season-1)
fcast <- forecast(fit.beer)
autoplot(fcast) +
  ggtitle("Forecasts of beer production using regression") +
  xlab("Year") + ylab("megalitres")
@
\caption{Forecasts from the regression model for beer production. The dark shaded region shows 80\% prediction intervals and the light shaded region shows 95\% prediction intervals.}
\label{fig:beer}
\end{figure}


{\bf Scenario based forecasting}: In this setting, the forecaster assumes possible scenarios for the predictor variables that are of interest. For example, a US policy maker may be interested in comparing the predicted change in consumption when there is a constant growth of 1\% and 0.5\% respectively for income and savings with no change in the employment rate, versus a respective decline of 1\% and 0.5\%, for each of the four quarters following the end of the sample. 

<<Spurious regression>>=
fit.consBest <- tslm(
  Consumption ~ Income + Savings + Unemployment,
  data = uschange)
h <- 4
newdata <- data.frame(
    Income = c(1, 1, 1, 1),
    Savings = c(0.5, 0.5, 0.5, 0.5),
    Unemployment = c(0, 0, 0, 0))
fcast.up <- forecast(fit.consBest, newdata = newdata)
newdata <- data.frame(
    Income = rep(-1, h),
    Savings = rep(-0.5, h),
    Unemployment = rep(0, h))
fcast.down <- forecast(fit.consBest, newdata = newdata)
@

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
autoplot(uschange[, 1]) +
  ylab("% change in US consumption") +
  autolayer(fcast.up, PI = TRUE, series = "increase") +
  autolayer(fcast.down, PI = TRUE, series = "decrease") +
  guides(colour = guide_legend(title = "Scenario"))
@
\caption{Forecasting percentage changes in personal consumption expenditure for the US under scenario based forecasting.}
\label{fig:beer2}
\end{figure}

\end{document}