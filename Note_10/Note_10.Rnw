\documentclass{article}
\usepackage{amsmath}
\newtheorem{exmp}{Example}[section]
\begin{document}
\SweaveOpts{concordance=TRUE}
\title{Forecasting}
\date{}
\maketitle
\section{Vector autoregressions}

\begin{itemize}
  \item One limitation with the models we have considered so far is that they impose a unidirectional relationship --- the forecast variable is influenced by the predictor variables, but not vice versa. However, there are many cases where the reverse should also be allowed for --- where all variables affect each other.
  \item Such feedback relationships are allowed for in the vector autoregressive (VAR) framework. In this framework, all variables are treated symmetrically. They are all modelled as if they influence each other equally. In more formal terminology, all variables are now treated as ``endogenous". To signify this we now change the notation and write all variables as $y$s: $y_{1,t}$ denotes the $t$th observation of variable $y_1$, $y_{2,t}$ denotes the $t$th observation of variable $y_2$, and so on.

\end{itemize}

Two examples:
\begin{itemize}
  \item In `uschange' example, the changes in personal consumption expenditure ($C_t$) were forecast based on the changes in personal disposable income ($I_t$). However, in this case a bi-directional relationship may be more suitable: an increase in $I_t$ will lead to an increase in $C_t$ and vice versa.
  \item An example of such a situation occurred in Australia during the Global Financial Crisis of 2008--2009. The Australian government issued stimulus packages that included cash payments in December 2008, just in time for Christmas spending. As a result, retailers reported strong sales and the economy was stimulated. Consequently, incomes increased.
\end{itemize}

 



\subsection{VAR model}
A VAR model is a generalisation of the univariate autoregressive model for forecasting a collection of variables; that is, a vector of time series.The relative simplicity of VARs has led to their dominance in forecasting.  We write a 2-dimensional VAR(1) as
\begin{align}
\label{var1a}
  y_{1,t} &= c_1+\phi _{11,1}y_{1,t-1}+\phi _{12,1}y_{2,t-1}+e_{1,t} \\
  y_{2,t} &= c_2+\phi _{21,1}y_{1,t-1}+\phi _{22,1}y_{2,t-1}+e_{2,t} 
\end{align}
where $e_{1,t}$ and $e_{2,t}$ are white noise processes that may be contemporaneously correlated.
\begin{itemize}
  \item If the series modelled are stationary we forecast them by directly fitting a VAR to the data (known as a ``VAR in levels").
  \item  If the series are non-stationary we take differences to make them stationary and then we fit a VAR model (known as a ``VAR in differences"). 
\end{itemize}

In both cases, the models are estimated equation by equation using the principle of least squares. For each equation, the parameters are estimated by minimising the sum of squared $e_{i,t}$ values.


\subsection{Forecasts from VAR}
Forecasts are generated from a VAR in a recursive manner. The VAR generates forecasts for *each* variable included in the system. 

To illustrate the process, assume that we have fitted the 2-dimensional VAR(1), for all observations up to time $T$. 

\begin{itemize}
  \item The one-step-ahead forecasts are generated by
\begin{align*}
  \hat y_{1,T+1|T} &=\hat{c}_1+\hat\phi_{11,1}y_{1,T}+\hat\phi_{12,1}y_{2,T} \\
  \hat y_{2,T+1|T} &=\hat{c}_2+\hat\phi _{21,1}y_{1,T}+\hat\phi_{22,1}y_{2,T}.
\end{align*}

Except that the errors have been set to zero and parameters have been replaced with their estimates.
\item For $h=2$, the forecasts are given by
\begin{align*}
  \hat y_{1,T+2|T} &=\hat{c}_1+\hat\phi_{11,1}\hat y_{1,T+1}+\hat\phi_{12,1}\hat y_{2,T+1}\\
  \hat y_{2,T+2|T}&=\hat{c}_2+\hat\phi_{21,1}\hat y_{1,T+1}+\hat\phi_{22,1}\hat y_{2,T+1}.
\end{align*}
\end{itemize}


\subsection{Practical considerations}
There are two decisions one has to make when using a VAR to forecast. They are: 
\begin{itemize}
  \item how many variables (denoted by $K$) 
  \item how many lags (denoted by $p$) should be included in the system. 
\end{itemize}

Further considerations:
\begin{itemize}
  \item The number of coefficients to be estimated in a VAR is equal to $K+pK^2$ (or $1+pK$ per equation). The more coefficients to be estimated the larger the estimation error entering the forecast.
  \item In practice it is usual to keep $K$ small and include only variables that are correlated to each other and therefore useful in forecasting each other. Information criteria are commonly used to select the number of lags to be included.
\end{itemize}



VARs are implemented in the **vars** package in R. It contains a function `VARselect` to choose the number of lags $p$ using four different information criteria: AIC, HQ, SC and FPE. SC is simply another name for the BIC (SC stands for Schwarz Criterion after Gideon Schwarz who proposed it). HQ is the Hannan-Quinn criterion and FPE is the ``Final Prediction Error" criterion. Care should be taken using the AIC as it tends to choose large numbers of lags. Instead, for VAR models, we prefer to use the BIC.

\subsection{Criticism}
A criticism VARs face is that they are atheoretical. They are not built on some economic theory that imposes a theoretical structure to the equations. Every variable is assumed to influence every other variable in the system, which makes direct interpretation of the estimated coefficients very difficult. Despite this, VARs are useful in several contexts:

\begin{itemize}
  \item forecasting a collection of related variables where no explicit interpretation is required;
  \item testing whether one variable is useful in forecasting another (the basis of Granger causality tests);
  \item impulse response analysis, where the response of one variable to a sudden but temporary change in another variable is analysed;
  \item forecast error variance decomposition, where the proportion of the forecast variance of one variable is attributed to the effect of other variables.
\end{itemize}

A more flexible generalisation would be a Vector ARMA process. However, the relative simplicity of VARs has led to their dominance in forecasting. Interested readers may refer to Athanasopoulos, Poskitt, and Vahid (2012).

Athanasopoulos, G., and Vahid, F. (2008). A complete VARMA modelling methodology based on scalar components. Journal of Time Series Analysis, 29(3), 533--554.
\subsection{Example}

There is a large discrepancy between a VAR(5) selected by the AIC and a VAR(1) selected by the BIC. As a result we first fit a VAR(1), selected by the BIC. In similar fashion to the univariate ARIMA methodology we test that the residuals are uncorrelated using a Portmanteau test (The tests for serial correlation in the ``vars" package are multivariate generalisations of the Ljung-Box tests). The null hypothesis of no serial correlation in the residuals is rejected for both a VAR(1) and a VAR(2) and therefore we fit a VAR(3) as now and the null is not rejected. The forecasts generated by the VAR(3) are plotted in Figure \ref{fig:var}.

<<msts>>=library(fpp2)
library(vars)
library(fpp2)
data(uschange)
VARselect(uschange[,1:2], lag.max=8, type="const")[["selection"]]
var <- VAR(uschange[,1:2], p=3, type="const")
serial.test(var, lags.pt=10, type="PT.asymptotic")
summary(var)
@


\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=

forecast(var) %>%
  autoplot() + xlab("Year")
@
\caption{Forecasts for US consumption and income generated from a VAR(3).}
\label{fig:var}
\end{figure}

\section{Neural network models}



\subsection{ Neural network architecture}

A neural network can be thought of as a network of ``neurons" organised in layers. The predictors (or inputs) form the bottom layer, and the forecasts (or outputs) form the top layer. There may be intermediate layers containing ``hidden neurons".

\begin{itemize}
  \item The very simplest networks contain no hidden layers and are equivalent to linear regression. Figure \ref{fig:fig-10-nnet} shows the neural network version of a linear regression with four predictors. 
  \begin{itemize}
  \item The coefficients attached to these predictors are called ``weights". 
  \item The forecasts are obtained by a linear combination of the inputs.
  \item  The weights are selected in the neural network framework using a ``learning algorithm" that minimises a ``cost function" such as MSE. 
  \item In this simple example, we can use linear regression which is a much more efficient method for training the model.

\end{itemize}
  \begin{figure}
  \includegraphics[width=\linewidth]{NNET1}
  \caption{ A simple neural network equivalent to a linear regression.}
  \label{fig:fig-10-nnet}
\end{figure}
\item Once we add an intermediate layer with hidden neurons, the neural network becomes non-linear. A simple example is shown in Figure \ref{fig:fig-10-nnet1}. 
\begin{itemize}
  \item This is known as a *multilayer feed-forward network* where each layer of nodes receives inputs from the previous layers.
  \item The outputs of nodes in one layer are inputs to the next layer.
  \item The inputs to each node are combined using a weighted linear combination.
  \item The result is then modified by a nonlinear function before being output. 

\item  For example, the inputs into hidden neuron $j$ in Figure \ref{fig:fig-10-nnet1} are linearly combined to give
$$
  z_j = b_j + \sum_{i=1}^4 w_{i,j} x_i.
$$
In the hidden layer, this is then modified using a nonlinear function such as a sigmoid,
$$
  s(z) = \frac{1}{1+e^{-z}},
$$
to give the input for the next layer. This tends to reduce the effect of extreme input values, thus making the network somewhat robust to outliers.
\item The parameters $b_1,b_2,b_3$ and $w_{1,1},\dots,w_{4,3}$ are ``learned" from the data. The values of the weights are often restricted to prevent them becoming too large. The parameter that restricts the weights is known as the ``decay parameter" and is often set to be equal to 0.1.
\item The weights take random values to begin with, which are then updated using the observed data. Consequently, there is an element of randomness in the predictions produced by a neural network. Therefore, the network is usually trained several times using different random starting points, and the results are averaged.
\item The number of hidden layers, and the number of nodes in each hidden layer, must be specified in advance. We will consider how these can be chosen using cross-validation later in this chapter.


\end{itemize}
\begin{figure}
  \includegraphics[width=\linewidth]{NNET2}
  \caption{A neural network with four inputs and one hidden layer with three hidden neurons.}
  \label{fig:fig-10-nnet1}
\end{figure}


\item The `avNNet` function from the **caret** package fits a feed-forward neural network with one hidden layer. The network specified here contains three nodes (`size=3`) in the hidden layer. The decay parameter has been set to 0.1. The argument `repeats=25` indicates that 25 networks were trained and their predictions are to be averaged. The argument `linout=TRUE` indicates that the output is obtained using a linear function. In this book, we will always specify `linout=TRUE`.

\end{itemize}











\subsection{Neural network autoregression}

\begin{itemize}
  \item With time series data, lagged values of the time series can be used as inputs to a neural network. 
\item In this note, we only consider feed-forward networks with one hidden layer, and use the notation NNAR($p,k$) to indicate there are $p$ lagged inputs and $k$ nodes in the hidden layer. 
\item With seasonal data, it is useful to also add the last observed values from the same season as inputs.
\begin{itemize}
\item For example, an NNAR(3,1,2)$_{12}$ model has inputs $y_{t-1}$, $y_{t-2}$, $y_{t-3}$ and $y_{t-12}$, and two neurons in the hidden layer.
\item An NNAR($p,P,k$)$_m$ model has inputs $$(y_{t-1},y_{t-2},\dots,y_{t-p},y_{t-m},y_{t-2m},y_{t-Pm})$$ and $k$ neurons in the hidden layer.
\end{itemize}
\item The `nnetar()' function fits an NNAR($p,P,k$)$_m$ model. If the values of $p$ and $P$ are not specified, they are automatically selected. \begin{itemize}
\item For non-seasonal time series, the default is the optimal number of lags (according to the AIC) for a linear AR($p$) model. For seasonal time series, the default values are $P=1$ and $p$ is chosen from the optimal linear model fitted to the seasonally adjusted data.
\item   If $k$ is not specified, it is set to $k=(p+P+1)/2$ (rounded to the nearest integer).

\end{itemize}
\end{itemize}


\begin{exmp}
The surface of the sun contains magnetic regions that appear
as dark spots. These affect the propagation of radio waves and so
telecommunication companies like to predict sunspot activity in order to
plan for any future difficulties. Sunspots follow a cycle of length
between 9 and 14 years. 
\end{exmp}
\begin{itemize}
\item In Figure \ref{fig:sunspotnnetar}, forecasts
from an NNAR(9,5) are shown for the next 20 years.
  \item The forecasts actually go slightly negative. If we wanted to restrict the forecasts to remain positive, we could use a log transformation (specified by the Box-Cox parameter $\lambda=0$) (Figure \ref{fig:sunspotnnetar2}).

\end{itemize}

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(caret)
data(sunspotarea)
fit <- nnetar(sunspotarea)
autoplot(forecast(fit,h=20))
@
\caption{Forecasts from a neural network with nine lagged inputs and one hidden layer containing five neurons.}
\label{fig:sunspotnnetar}
\end{figure}


\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
library(caret)
fit <- nnetar(sunspotarea, lambda=0)
autoplot(forecast(fit,h=20))
@
\caption{Forecasts of sun spots from a neural network with log transformation.}
\label{fig:sunspotnnetar2}
\end{figure}

\subsection{Prediction intervals}
Neural networks are not based on a well-defined stochastic model, and so it is not straightforward to derive prediction intervals for the resultant forecasts. However, we can still do it using simulation where future sample paths are generated using bootstrapped residuals.

\vspace{1cm}

\begin{exmp}
Suppose we fit a NNETAR model to the famous Canadian `lynx` data. We have used a Box-Cox transformation with $\lambda=0.5$ to ensure the residuals will be roughly homoscedastic (i.e., have constant variance).
\end{exmp}

<<msts>>=library(fpp2)
data(lynx)
(fit <- nnetar(lynx, lambda=0.5))
@


\begin{itemize}
  \item The model can be written as
$$
  y_t = f(\boldsymbol{y}_{t-1}) + \varepsilon_t
$$
where $\boldsymbol{y}_{t-1} = (y_{t-1},y_{t-2},\dots,y_{t-8})'$ is a vector containing lagged values of the series, and $f$ is a neural network with 4 hidden nodes in a single layer.
\item The error series $\{\varepsilon_t\}$ is assumed to be homoscedastic (and possibly also normally distributed).
\item We can simulate future sample paths of this model iteratively, by randomly generating a value for $\varepsilon_t$, either from a normal distribution, or by resampling from the historical values. So if $\varepsilon^*_{T+1}$ is a random draw from the distribution of errors at time $T+1$, then
$$
  y^*_{T+1} = f(\boldsymbol{y}_{T}) + \varepsilon^*_{T+1}
$$
is one possible draw from the forecast distribution for $y_{T+1}$. Setting 
$\boldsymbol{y}_{T+1}^* = (y^*_{T+1}, y_{T}, \dots, y_{T-6})'$, we can then repeat the process to get
$$
  y^*_{T+2} = f(\boldsymbol{y}^*_{T+1}) + \varepsilon^*_{T+2}.
$$
\item In this way, we can iteratively simulate a future sample path. By repeatedly simulating sample paths, we build up knowledge of the distribution for all future values based on the fitted neural network. Here is a simulation of 9 possible future sample paths for the lynx data. Each sample path covers the next 20 years after the observed data.
<<msts>>=library(fpp2)
sim <- ts(matrix(0, nrow=20, ncol=9), start=end(lynx)[1]+1)
for(i in seq(9))
  sim[,i] <- simulate(fit, nsim=20)
autoplot(lynx) + forecast::autolayer(sim)

@

\item 

If we do this a few hundred or thousand times, we can get a very good picture of the forecast distributions. This is how the `forecast.nnetar` function produces prediction intervals:

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
fcast <- forecast(fit, PI=TRUE, h=20)
autoplot(fcast)
@
\caption{Future sample paths for the lynx data.}
\label{fig:var}
\end{figure}
\item Because it is a little slow, `PI=FALSE` is the default, so prediction intervals are not computed unless requested. The `npaths` argument in `forecast.nnetar` controls how many simulations are done (default 1000). By default, the errors are drawn from a normal distribution. The `bootstrap` argument allows the errors to be `bootstrapped" (i.e., randomly drawn from the historical errors). 

\end{itemize}


\section{Bootstrapping and bagging}

\subsection{Bootstrapping}
First it helps us to get a better measure of forecast uncertainty, and second it provides a way of improving our point forecasts using ``bagging''.
\subsubsection{The Bootstrapping procedure}
\begin{enumerate}
  \item The time series is Box-Cox-transformed, and then decomposed into trend, seasonal and remainder components using STL.
 \item We use a ``blocked bootstrap'', where contiguous sections of the time series are selected at random and joined together.
 \item These bootstrapped remainder series are added to the trend and seasonal components, and the Box-Cox transformation is reversed to give variations on the original time series.
\end{enumerate}
\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
bootseries <- bld.mbb.bootstrap(debitcards, 10) %>%
  as.data.frame() %>% ts(start=2000, frequency=12)
autoplot(debitcards) +
  autolayer(bootseries, colour=TRUE) +
  autolayer(debitcards, colour=FALSE) +
  ylab("Bootstrapped series") + guides(colour="none")
@
\caption{Ten bootstrapped versions of monthly expenditure on retail debit cards in Iceland.}
\label{fig:bootstrap}
\end{figure}

\newpage
\subsubsection{Prediction interval}

\begin{itemize}
\item Almost all prediction intervals from time series models are too narrow. This is a well-known phenomenon and arises because they do not account for all sources of uncertainty. 
  \item Hyndman, Koehler, Snyder, and Grose (2002) measured the size of the problem by computing the actual coverage percentage of the prediction intervals on test data, and found that for ETS models, nominal 95\% intervals may only provide coverage between 71\% and 87\%. The difference is due to missing sources of uncertainty (for example, 3 and 4 below).
  \begin{enumerate}
  \item The random error term;
\item The parameter estimates;
\item The choice of model for the historical data;
\item The continuation of the historical data generating process into the future.
\end{enumerate}
\item Using Bootstrap method to overcome the problem:
\begin{enumerate}
  \item First, we simulate many time series that are similar to the original data, using the block-bootstrap described above.
  \item For each of these series, we fit an ETS model and simulate one sample path from that model. 
  \item Finally, we take the means and quantiles of these simulated sample paths to form point forecasts and prediction intervals.
\end{enumerate}

\end{itemize}


<<msts>>=library(fpp2)
data(debitcards)
nsim <- 50L
sim <- bld.mbb.bootstrap(debitcards, nsim)
h <- 36L
future <- matrix(0, nrow=nsim, ncol=h)
for(i in seq(nsim))
  future[i,] <- simulate(ets(sim[[i]]), nsim=h)
start <- tsp(debitcards)[2]+1/12
simfc <- structure(list(
    mean = ts(colMeans(future), start=start, frequency=12),
    lower = ts(apply(future, 2, quantile, prob=0.025),
               start=start, frequency=12),
    upper = ts(apply(future, 2, quantile, prob=0.975),
               start=start, frequency=12),
    level=95),
  class="forecast")
@

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
etsfc <- forecast(ets(debitcards), h=h, level=95)
autoplot(debitcards) +
  ggtitle("Monthly retail debit card usage in Iceland") +
  xlab("Year") + ylab("million ISK") +
  autolayer(simfc, series="Simulated") +
  autolayer(etsfc, series="ETS")
@
\caption{Forecasts of Iceland debit card usage using an ETS model with regular prediction intervals along with prediction intervals computed using simulations that allow for model and parameter uncertainty.}
\label{fig:bootstrap2}
\end{figure}
Hyndman, R. J., Koehler, A. B., Snyder, R. D., and Grose, S. (2002). A state space framework for automatic forecasting using exponential smoothing methods. International Journal of Forecasting, 18(3), 439--454.
\subsection{Bagging}

If we produce forecasts from each of the additional time series, and average the resulting forecasts, we get better forecasts than if we simply forecast the original time series directly. This is called ``bagging'' which stands for ``bootstrap aggregating''.
\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
sim <- bld.mbb.bootstrap(debitcards, 10) %>%
  as.data.frame() %>%
  ts(frequency=12, start=2000)
fc <- purrr::map(as.list(sim),
           function(x){forecast(ets(x))[["mean"]]}) %>%
      as.data.frame() %>%
      ts(frequency=12, start=start)
autoplot(debitcards) +
  autolayer(sim, colour=TRUE) +
  autolayer(fc, colour=TRUE) +
  autolayer(debitcards, colour=FALSE) +
  ylab("Bootstrapped series") +
  guides(colour="none")
@
\caption{Forecasts of the ten bootstrapped series obtained using ETS models.}
\label{fig:boost}
\end{figure}


\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
etsfc <- debitcards %>% ets() %>% forecast(h=36)
baggedfc <- debitcards %>% baggedETS() %>% forecast(h=36)
autoplot(debitcards) +
  autolayer(baggedfc, series="BaggedETS", PI=FALSE) +
  autolayer(etsfc, series="ETS", PI=FALSE) +
  guides(colour=guide_legend(title="Forecasts"))
@
\caption{Comparing bagged ETS forecasts (the average of 100 bootstrapped forecast) and ETS applied directly to the data.}
\label{fig:boost}
\end{figure}

\section{Forecasting very long time series}
\begin{itemize}
  \item Most time series models do not work well for very long time series. 
  \begin{itemize}
  \item Real data do not come from the models we use.
  \item When the number of observations is not large (say up to about 200) the models often work well as an approximation to whatever process generated the data. 
  \item As sample increases, the difference between the true process and the model starts to become more obvious. 
  \item Optimization of the parameters becomes more time consuming because of the number of observations involved.
\end{itemize}
\item What to do about these issues depends on the purpose of the model. 
\begin{itemize}
  \item A more flexible and complicated model could be used, but this still assumes that the model structure will work over the whole period of the data.
  \item A better approach is usually to allow the model itself to change over time. 
  \begin{itemize}
  \item ETS models are designed to handle this situation by allowing the trend and seasonal terms to evolve over time.
  \item ARIMA models with differencing have a similar property. 
  \item Dynamic regression models do not allow any evolution of model components.
\end{itemize}
 \item If we are only interested in forecasting the next few observations, one simple approach is to throw away the earliest observations and only fit a model to the most recent observations. 
\end{itemize}


\end{itemize}

\section{Missing values and outliers}
Real data often contains missing values, outlying observations, and other messy features.
\subsection{Missing values}
\begin{itemize}
  \item Missing data can arise for many reasons, and it is worth considering whether the missingness will induce bias in the forecasting model. For example, suppose we are studying sales data for a store, and missing values occur on public holidays when the store is closed. The following day may have increased sales as a result.\begin{itemize}
  \item   If we fail to allow for this in our forecasting model, we will most likely under-estimate sales on the first day after the public holiday, but over-estimate sales on the days after that. 
  \item One way to deal with this kind of situation is to use a dynamic regression model, with dummy variables indicating if the day is a public holiday or the day after a public holiday. 
  \end{itemize}
\item The missingness may be essentially random. If the timing of the missing data is not informative for the forecasting problem, then the missing values can be handled more easily.\begin{itemize}
  \item The naive forecasting method continues to work, with the most recent non-missing value providing the forecast for the future time periods.
  \item The R functions for ARIMA models, dynamic regression models and NNAR models will also work correctly without causing errors. 
  \item ets(), stlf(), and tbats() do not automatically handle missing values.
\end{itemize}
\item Two ways of handling missing values:
\begin{itemize}
  \item First, we could just take the section of data after the last missing value, assuming there is a long enough series of observations to produce meaningful forecasts. 
  \item Alternatively, we could replace the missing values with estimates. The na.interp() function is designed for this purpose. For non-seasonal data like this, simple linear interpolation is used to fill in the missing sections. For seasonal data, an STL decomposition is used estimate the seasonally component, and the seasonally adjusted series are linear interpolated. More sophisticated missing value interpolation is provided in the imputeTS package.

\end{itemize}

\end{itemize}

\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
data(gold)
gold2 <- na.interp(gold)
autoplot(gold2, series="Interpolated") +
  autolayer(gold, series="Original") +
  scale_colour_manual(
    values=c(`Interpolated`="red",`Original`="gray"))
@
\caption{Daily morning gold prices for 1108 consecutive trading days beginning on 1 January 1985 and ending on 31 March 1989.}
\label{fig:missing}
\end{figure}

\subsection{Outliers}
Outliers are observations that are very different from the majority of the observations in the time series. They may be errors, or they may simply be unusual. 

\begin{itemize}
\item All of the methods we have considered will not work well if there are extreme outliers in the data. In this case, we may wish to replace them with missing values, or with an estimate that is more consistent with the majority of the data.

  \item The tsoutliers() function is designed to identify outliers, and to suggest potential replacement values.  Residuals are identified by fitting a loess curve for non-seasonal data and via a periodic STL decomposition for seasonal data. Residuals are labelled as outliers if they lie outside the range $\pm 2(q_{0.9}-q_{0.1})$
 where $q_p$ is the $p$-quantile of the residuals. For a Gaussian distribution, it will identify less than 1 point in 3 million as an outlier.
  
<<msts>>=library(fpp2)
tsoutliers(gold)
gold[768:772]
@
\item Simply replacing outliers without thinking about why they have occurred is a dangerous practice.
\item However, if we are willing to assume that the outliers are genuinely errors, or that they won't occur in the forecasting period, then replacing them can make the forecasting task easier.
\item tsclean() identifies and replaces outliers, and also replaces missing values. 
\begin{figure}
<<eval=TRUE,fig=TRUE,echo=TRUE,fig.width=4,fig.align='center'>>=
gold %>%
  tsclean() %>%
  ets() %>%
  forecast(h=50) %>%
  autoplot()
@
\caption{ Forecasts from an ETS model for the gold price data after removing an outlier.}
\label{fig:outlier}
\end{figure}

\item Check out `tsoutlier' package about additive outliers and innovative outliers.
\end{itemize}

Chen, C. and Liu, Lon-Mu (1993). Joint Estimation of Model Parameters and Outlier Effects in
Time Series. Journal of the American Statistical Association, 88(421), pp. 284--297. 

\end{document}